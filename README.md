# OpenAlex Relational Parser

This project converts the OpenAlex snapshot (JSON Lines files) into `88` CSV files that match the CWTS relational schema. The CSV outputs are ready to load into a relational database such as SQL Server.

> The OpenAlex relational schema was designed by Dr. Nees Jan van Eck and his colleagues at Leiden University CWTS. They deserve full credit for that work. The maintainers of this repository are solely responsible for the parsing code presented here.

## Repository Layout

```
openalex-relational-parser/
|-- data/
|   |-- openalex-snapshot-YYYYMMDD/      # OpenAlex JSON snapshot (gzip files)
|   `-- reference/
|       |-- openalex_cwts_schema.sql     # CWTS schema definition
|       `-- openalex_cwts_sample_export  # Reference CSV samples and enumerations
|-- src/
|   `-- openalex_parser/
|       |-- cli.py                       # Command-line entry point
|       |-- transformers/                # JSON to CSV mappers for each entity
|       `-- ...                          # Shared utilities, schema loader, etc.
`-- output/                              # CSV files generated by the converter (created after running)
```

## Installation

The converter only depends on the Python standard library. Python 3.9 or newer is recommended (tested with CPython on Windows).

## Usage

Run the CLI module from the repository root:

```
python -m openalex_parser.cli --entity all --output-dir output
```

The command will:

1. Read the CWTS schema (`data/reference/openalex_cwts_schema.sql`).
2. Load enumerations from `data/reference/openalex_cwts_sample_export`.
3. Iterate over every OpenAlex entity under `data/openalex-snapshot-YYYYMMDD/data`.
4. Emit 88 CSV files into the specified output directory.
5. Print per-entity progress and a summary when finished.

### Optional Arguments

- `--schema PATH` - Override the schema SQL file location.
- `--reference-dir PATH` - Override the directory that contains enumeration CSVs.
- `--snapshot PATH` - Root of the OpenAlex snapshot (expects subfolders such as `works/`, `authors/`, ...).
- `--output-dir PATH` - Target directory for CSV exports (default `output`).
- `--encoding {utf-8,utf-16le}` - Encoding for generated CSV files (default `utf-8`).
- `--delimiter CHAR` - Single-character delimiter for CSV output (default `,`; pass `\t` for tab-separated values).
- `--entity NAME` - Process a subset of entities; repeat the flag to list multiple names. Use `--entity all` to run the full pipeline.
- `--updated-date YYYY-MM-DD` - Restrict processing to specific `updated_date=` partitions (repeatable).
- `--max-records N` - Limit records per entity (handy for smoke tests). Omit or set to zero for full runs.
- `--max-files N` - Limit the number of gzip part files per entity.
- `--progress-interval N` - Records between progress messages (default `1000`).

#### Examples

Process a small slice of the `works` entity:

```
python -m openalex_parser.cli \
    --entity works \
    --updated-date 2023-05-17 \
    --max-records 500 \
    --output-dir staging-output
```

Process authors and institutions only:

```
python -m openalex_parser.cli --entity authors --entity institutions --output-dir output
```

## Output

The CLI writes one CSV per table defined in the CWTS schema. File names exactly match table names (for example `work.csv`, `work_author.csv`, ...). Columns appear in schema order and headers match the column names. Shared lookup tables (countries, keywords, SDGs, etc.) are de-duplicated automatically.

## Field Notes and Post-processing

- Some analytical fields from the CWTS schema (for example, citation windows or self-citation flags) require cross-record aggregation. The converter leaves those fields blank so they can be populated in a downstream analytics step if needed.
- `work_detail.author_et_al` collapses long author lists by showing the first three names, an ellipsis marker, and the final name when more than four authors are present.
- Enumeration tables (work types, OA status, source types, etc.) are populated on the fly. New values observed in the snapshot receive deterministic IDs via the bundled stable ID generator.

## Troubleshooting

- **Missing data directories:** Ensure `data/openalex-snapshot-20250930/data` contains the OpenAlex gzip folders (works, authors, institutions, and so on). The CLI skips entities whose directories are absent.
- **Slow smoke tests:** Use `--max-records` and `--max-files` to reduce runtime while verifying changes.
- **Character encoding:** CSV files default to UTF-8 with Unix-style newlines; use `--encoding utf-16le` if a different encoding is required.

## Next Steps

- Integrate post-processing steps (for example, citation window calculations) as needed.
- Extend automated validation by comparing generated CSV headers against the reference samples.

Happy parsing!
